<!DOCTYPE html>
<html>
<link href="https://fonts.cdnfonts.com/css/chalkduster" rel="stylesheet">
<style>
    @import url('https://fonts.cdnfonts.com/css/chalkduster');
</style>

<head>
    <!--<script src="https://code.jquery.com/jquery-1.11.2.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.14.7/dist/umd/popper.min.js"></script>
    <script src="https://code.jquery.com/ui/1.13.2/jquery-ui.min.js"></script>-->

	<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
	<script src="https://cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
	<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

    <!--<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/js/bootstrap.min.js"></script>-->
    <!--<script src="http://www.google.com/jsapi" type="text/javascript"></script>-->

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css">

    <link href="styles.css" rel="stylesheet" />
    <link href="fontawesome.all.min.css" rel="stylesheet" />
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
	<script defer src="fontawesome.all.min.js"></script>

    <script src="script.js"></script>

    <link rel="icon" type="image/png" href="./assests/afine_logo.png" />

    <title>Toward Generalized Image Quality Assessment: Relaxing the Perfect Reference Quality Assumption</title>
    <meta property="og:image" content="./webpage_assets/teaser.jpg">
    <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
    <meta property="og:title" content="Toward Generalized Image Quality Assessment: Relaxing the Perfect Reference Quality Assumption">
    <meta property="og:description" content="A large-scale database and a generalized FR-IQA model that evaluates image quality in both perfect and imperfect reference conditions.">

    <!-- Get from Google Analytics -->
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async="" src=""></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'UA-75863369-6');
    </script>
</head>

<body>
    <br>
    <center>
        <span class="near-black" style="font-size:36px;font-weight:bold">Toward Generalized Image Quality Assessment:</span>
        <br>
		<span class="near-black" style="font-size:36px;font-weight:bold">Relaxing the Perfect Reference Quality Assumption</span>
        <br>
		<span class="near-black" style="font-size:32px;">CVPR 2025</span>
        <br>
        <table align="center" width="900px">
            <tbody>
                <tr>
                    <td align="center" width="215px">
                        <center>
                            <span style="font-size:22px"><a href="https://github.com/ChrisDud0257">Du
                                    Chen</a><sup style="vertical-align: sub;padding-left: 2px">*</sup><sup style="font-size: 0.7em;">1,3</sup>
                                </span>
                        </center>
                    </td>
                    <td align="center" width="235px">
                        <center>
                            <span style="font-size:22px"><a href="https://tianhewu.github.io/tianhe-page/">Tianhe 
                                    Wu</a><sup style="vertical-align: sub;padding-left: 2px">*</sup><sup
                                        style="font-size: 0.7em;">2,3</sup></span>
                        </center>
                    </td>
                    <td align="center" width="260px">
                        <center>
                            <span style="font-size:22px"><a href="https://kedema.org/">Kede 
                                    Ma</a><sup style="font-size: 0.7em;">2</sup></span>
                        </center>
                    </td>
                    <td align="center" width="190px">
                        <center>
                            <span style="font-size:22px"><a href="https://www4.comp.polyu.edu.hk/~cslzhang/">Lei
                                    Zhang</a><sup style="font-size: 0.7em;padding-left: 1px;">1,3</sup></span>
                        </center>
                    </td>
                </tr>
            </tbody>
        </table>
        <table align="center" width="600px">
            <tbody>
                <tr>
                    <td style="padding:10px">
						<sup style="padding-right: 2px;font-size: 18px;top: -0.1em;">1</sup><img style="height:50px" src="./assests/polyu_logo.png">
                    </td>
                    <td style="padding:10px">
						<sup style="padding-right: 2px;font-size: 18px;top: -0.1em;">2</sup><img style="height:50px" src="./assests/cityu_logo.jpg">
                    </td>
                    <td style="padding:10px">
						<sup style="padding-right: 2px;font-size: 18px;top: -0.1em;">3</sup><img style="height:50px" src="./assests/oppo_logo.png">
                    </td>
                </tr>
            </tbody>
        </table>
        <table align="center" width="500px">
            <tbody>
                <tr>
                    <td align="center" width="500px">
                        <center>
                            <i><span style="font-size:14px">* Equal contribution, order decided by random
                                    seed</span></i>
                        </center>
                    </td>
                </tr>
            </tbody>
        </table>
        <table width="800" border="0" align="center" class="menu"
            style="margin-bottom: 20px;margin-top: 20px;font-weight: bold;font-size: 20px;">
            <tbody>
                <tr>
                    <td align="center">
						<!-- <span class="bg-near-black f6 no_hover br-pill ph3 pv2 dib" style="vertical-align:top;height:36px">
							<a style="vertical-align:sub;color:inherit;text-decoration:none;font-size:15px" href="https://arxiv.org/abs/2306.09344" target="_blank">
							<i style="padding-right:2px;font-size:20px" class="ai ai-arxiv" aria-hidden="true"></i>
							Arxiv
							</a>
						</span> -->
						<span class="bg-near-black f6 no_hover br-pill ph3 pv2 dib" style="vertical-align:top;height:36px">
							<a style="color:inherit;text-decoration:none;font-size:15px" href="https://github.com/ChrisDud0257/AFINE" target="_blank">
							<i style="padding-right:2px;font-size:23px" class="fab fa-github" aria-hidden="true"></i>
							Code
							</a>
						</span>
                        <!-- <span class="bg-near-black f6 no_hover br-pill ph3 pv2 dib" style="vertical-align:top;height:36px">
							<a style="vertical-align:sub;color:inherit;text-decoration:none;font-size:15px" href="https://try.fiftyone.ai/datasets/nights/samples" target="_blank">
							<img style="padding-bottom:5px;padding-right:2px;font-size:20px;height:22px" src="webpage_assets/voxel51.png" aria-hidden="true"></img>
							Explore the Dataset!
							</a>
						</span> -->
                        <!-- <span class="bg-near-black f6 no_hover br-pill ph3 pv2 dib" style="vertical-align:top;height:36px">
							<a style="vertical-align:sub;color:inherit;text-decoration:none;font-size:15px" href="#bibtex" target="_self">
							<i style="padding-right:2px;font-size:20px" class="fa fa-quote-left" aria-hidden="true"></i>
							Bibtex
							</a>
						</span> -->
                    </td>
                </tr>
            </tbody>
        </table>

    </center>

    <center>
        <img class="round" style="width:900px" src="./assests/teaser.svg">
    </center>
    <br>
	<p id="teaser-description">
        <span id="bold">With a reference image in the middle, which image, A or B, has better perceived visual quality?</span> 
		The proposed A-FINE generalizes and outperforms standard FR-IQA models under both perfect and imperfect reference conditions.
    </p>
	<br>
    <hr>

    <center>
        <h1>Abstract</h1>
    </center>
    <table align="center" width="900px">

        <tbody>
            <tr>
                <td>
                    Most full-reference image quality assessment (FR-IQA) models assume that the reference image is of perfect quality.
                    However, this assumption is flawed because many reference images in existing IQA datasets are of subpar quality.
                    Moreover, recent generative image enhancement methods are capable of producing images of higher quality than their original counterparts.
                    These factors challenge the effectiveness and applicability of current FR-IQA models.
                    To address this limitation, we build a large-scale IQA database, namely DiffIQA, 
                    which contains approximately 180,000 images generated by a diffusion-based image enhancer with adjustable hyper-parameters.
                    Each image is annotated by human subjects as either worse, similar, or better quality compared to its reference.
                    Building on this, we present a generalized FR-IQA model, namely <span id="bold">A</span>daptive 
                    <span id="bold">FI</span>delity-<span id="bold">N</span>aturalness <span id="bold">E</span>valuator (A-FINE),
                    to accurately assess and adaptively combine the fidelity and naturalness of the test image.
                    A-FINE aligns well with standard FR-IQA when the reference image is much more natural than the test image. 
                    We demonstrate by extensive experiments that A-FINE surpasses existing FR-IQA models on well-established IQA datasets and our newly created DiffIQA. 
                    To further validate A-FINE, we additionally construct a super-resolution IQA benchmark (SRIQA-Bench), 
                    encompassing test images derived from ten state-of-the-art SR methods with reliable human quality annotations. 
                    Tests on SRIQA-Bench re-affirm the advantages of A-FINE.
                </td>
            </tr>
        </tbody>
    </table>
    <br>
    <hr>
    <center>
        <h1>Installation and Usage</h1>
    </center>
    <div class="container-fluid" id="code-container" align="center" width="900px">
        <div class="row">
            <div class="col" id="pip-container" class="code-col" style="flex-direction: column;display:flex;" >
            <p>Installation:</p>
            <code>
                git clone https://github.com/ChrisDud0257/AFINE
                <br>
                cd QuickInference
                <br>
                conda create --name afine python=3.10
                <br><br>
                pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118
                <br>
                pip install -r requirements.txt
            </code>
			<br>
            <p>Usage:</p>
            <code>
                cd QuickInference
                <br><br>
                python afine.py --pretrain_CLIP_path [path to the pretrained CLIP ViT-B-32.pt] --afine_path [path to our afine.pth] \
                --dis_img_path [path to the distortion image] --ref_img_path [path to the reference image]
            </code>
			<br>
        </div>
    </div>
    <br>
    <hr>

    <center>
        <h1>Method</h1>
    </center>

    <center>
        <div class="container-fluid" id="method-container" align="center" width="900px">
            <div class="row">
                <div class="col" id="method-figure">
                    <img class="round" style="width:500px" src="./assests/A-FINE.svg">
                </div>
                <div class="col" id="method-description">
                    <span id="bold">A-FINE</span> leverages a shared feature transformation to perform image fidelity and
                     naturalness assessment, which are adaptively combined to produce the final quality score.
                </div>
            </div>
        </div>

    </center>
    <br>
    <hr>
    <center>
        <h1>DiffIQA</h1>
    </center>

    <p id="diffiqa-description">
        We gather original input images from three sources: 1) 1,200 images from the DF2K dataset;
        2) 1,000 images from the Internet under the license of Creative Commons; and 3) 
        640 images captured using mobile phones or digital cameras. 
        The input images are cropped to 512&times;512 with an overlap of less than 128 pixels, 
        leading to a total of 29,868 images as inputs to our trained enhancer. 
        During inference, we randomly 1) apply the same degradations as used during training, 2) 
        augment the initial image latent with additive Gaussian noise of varying intensities, and 3) 
        adjust the sampling steps within range [20, 1000] to generate images with diverse quality levels. 
        By combining these operations, we generate six test images for each input image, yielding a total of 179,208 test images.
    </p>

    <style>
        .flex-gallery {
            display: flex;
            flex-wrap: wrap;
            justify-content: center;
            gap: 5px;
            max-width: 100%;
            margin: auto;
        }
        .flex-gallery figure {
            width: calc((100% - 10px * 5) / 6); /* 计算适合 6 张图片的宽度 */
            text-align: center; /* 让 caption 文字居中 */
            margin: 0;
        }
        .flex-gallery img {
            width: 100%; /* 让图片填充 figure */
            height: auto;
            object-fit: cover;
        }
        .flex-gallery figcaption {
            font-size: 14px; /* 调整文字大小 */
            color: #555; /* 使文字颜色柔和 */
            margin-top: 3px; /* 图片和文字之间的间距 */
        }
    </style>
    
    <div class="flex-gallery">
        <figure>
            <img src="./assests/worse/0009x784y4401.png" alt="Image">
            <figcaption>Worse</figcaption>
        </figure>
        <figure>
            <img src="./assests/worse/0009x784y44Original.png" alt="Image">
            <figcaption>Reference</figcaption>
        </figure>
        <figure>
            <img src="./assests/worse/000011x398y81203.png" alt="Image">
            <figcaption>Worse</figcaption>
        </figure>
        <figure>
            <img src="./assests/worse/000011x398y812Original.png" alt="Image">
            <figcaption>Reference</figcaption>
        </figure>
        <figure>
            <img src="./assests/worse/000042x403y42301.png" alt="Image">
            <figcaption>Worse</figcaption>
        </figure>
        <figure>
            <img src="./assests/worse/000042x403y423Original.png" alt="Image">
            <figcaption>Reference</figcaption>
        </figure>
    
        <figure>
            <img src="./assests/similar/0030x393y77301.png" alt="Image">
            <figcaption>Similar</figcaption>
        </figure>
        <figure>
            <img src="./assests/similar/0030x393y773Original.png" alt="Image">
            <figcaption>Reference</figcaption>
        </figure>
        <figure>
            <img src="./assests/similar/0043x22y1901.png" alt="Image">
            <figcaption>Similar</figcaption>
        </figure>
        <figure>
            <img src="./assests/similar/0043x22y19Original.png" alt="Image">
            <figcaption>Reference</figcaption>
        </figure>
        <figure>
            <img src="./assests/similar/000310x788y001.png" alt="Image">
            <figcaption>Similar</figcaption>
        </figure>
        <figure>
            <img src="./assests/similar/000310x788y0Original.png" alt="Image">
            <figcaption>Reference</figcaption>
        </figure>
    
        <figure>
            <img src="./assests/better/0016x0y117301.png" alt="Image">
            <figcaption>Better</figcaption>
        </figure>
        <figure>
            <img src="./assests/better/0016x0y1173Original.png" alt="Image">
            <figcaption>Reference</figcaption>
        </figure>
        <figure>
            <img src="./assests/better/0024x32y77401.png" alt="Image">
            <figcaption>Better</figcaption>
        </figure>
        <figure>
            <img src="./assests/better/0024x32y774Original.png" alt="Image">
            <figcaption>Reference</figcaption>
        </figure>
        <figure>
            <img src="./assests/better/0024x785y116901.png" alt="Image">
            <figcaption>Better</figcaption>
        </figure>
        <figure>
            <img src="./assests/better/0024x785y1169Original.png" alt="Image">
            <figcaption>Reference</figcaption>
        </figure>
    </div>
    

    <br>
	<p id="diffiqa-description">
        Instances of images that are worse, similar, and better to their reference counterparts in our DiffIQA dataset.
    </p>

    <hr>
    <center>
        <h1>SRIQA-Bench</h1>
    </center>

    <p id="sriqa-bench-description">
        We first compile 100 original images covering a wide range of natural scenes and subject
        them to common degradations to generate input low-resolution images. 
        We then adopt two regressive SR methods: 1) SwinIR  and 2) RRDB, and eight generative SR methods: 3) Real-ESRGAN, 
        4) BSRGAN, 5) HGGT, 6) SUPIR, 7) SeeSR, 8) StableSR, 9) SinSR and 10) OSEDiff to produce ten SR images. 
        Generally speaking, diffusion-based SR methods outperform GAN-based methods with more plausible textures, 
        while generative SR methods are more effective than regressive SR methods with more realistic structures.
    </p>

    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>图片排列示例</title>
        <style>
            .gallery {
                display: grid;
                grid-template-columns: repeat(4, 1fr);
                gap: 5px;
                text-align: center;
            }
            .gallery img {
                width: 100%;
                height: auto;
            }
            .caption {
                font-size: 14px;
                margin-top: 5px;
            }
        </style>
    </head>
    <body>
    
    <div class="gallery">
        <div>
            <img src="./assests/sriqa_bench/realworld24_Original.png" alt="Reference">
            <div class="caption">Reference</div>
        </div>
        <div>
            <img src="./assests/sriqa_bench/realworld24_LR.png" alt="LR">
            <div class="caption">LR</div>
        </div>
        <div>
            <img src="./assests/sriqa_bench/realworld24_SwinIRx4.png" alt="SwinIR">
            <div class="caption">SwinIR</div>
        </div>
        <div>
            <img src="./assests/sriqa_bench/realworld24_RRDBx4.png" alt="RRDB">
            <div class="caption">RRDB</div>
        </div>
        <div>
            <img src="./assests/sriqa_bench/realworld24_RealESRGANx4.png" alt="RealESRGAN">
            <div class="caption">RealESRGAN</div>
        </div>
        <div>
            <img src="./assests/sriqa_bench/realworld24_BSRGANx4.png" alt="BSRGAN">
            <div class="caption">BSRGAN</div>
        </div>
        <div>
            <img src="./assests/sriqa_bench/realworld24_HGGTx4.png" alt="HGGT">
            <div class="caption">HGGT</div>
        </div>
        <div>
            <img src="./assests/sriqa_bench/realworld24_SUPIRx4.png" alt="SUPIR">
            <div class="caption">SUPIR</div>
        </div>
        <div>
            <img src="./assests/sriqa_bench/realworld24_SeeSRx4.png" alt="SeeSR">
            <div class="caption">SeeSR</div>
        </div>
        <div>
            <img src="./assests/sriqa_bench/realworld24_StableSRx4.png" alt="StableSR">
            <div class="caption">StableSR</div>
        </div>
        <div>
            <img src="./assests/sriqa_bench/realworld24_SinSRx4.png" alt="SinSR">
            <div class="caption">SinSR</div>
        </div>
        <div>
            <img src="./assests/sriqa_bench/realworld24_OSEDiffx4.png" alt="OSEDiff">
            <div class="caption">OSEDiff</div>
        </div>
    </div>
    
    </body>


    <hr>
    <center>
        <h1>Acknowledgements</h1>
        <table align="center" width="900px">
            <tbody>
                <tr>
                    <td>
                        This work was supported in part by the Hong Kong ITC Innovation and Technology Fund (9440379 and 9440390) and the PolyU-OPPO Joint Innovative Research Center. 
                        We sincerely thank the volunteers who took part in our subjective study. A Human Subjects Ethics Committee approved the study, and all
                        participants signed consent forms beforehand.
					</td>
                </tr>
            </tbody>
        </table>
    </center>
    <br>
    <hr>
    <center>
        <h1 name="bibtex" id="bibtex">Bibtex</h1>
    </center>
    <div class="col" class="code-col">
        <code style="font-size: 14px;">
            <!-- @inproceedings{fu2023dreamsim,<br>
                &ensp;title={DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data},<br>
                &ensp;author= {Fu, Stephanie and Tamir, Netanel and Sundaram, Shobhita and Chai, Lucy and Zhang, Richard and Dekel, Tali and Isola, Phillip},<br>
		&ensp;booktitle={Advances in Neural Information Processing Systems},<br>
                &ensp;pages={50742--50768},<br>
		&ensp;volume={36}<br>
		&ensp;year={2023}<br>
              } -->
        </code>
    </div>
    <br>
    <hr>
</body>

</html>
